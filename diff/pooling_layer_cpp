diff --git a/src/caffe/layers/pooling_layer.cpp b/src/caffe/layers/pooling_layer.cpp
index 6f4c69c..9f6cb30 100644
--- a/src/caffe/layers/pooling_layer.cpp
+++ b/src/caffe/layers/pooling_layer.cpp
@@ -37,6 +37,11 @@ void PoolingLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,
       && pool_param.has_stride_w())
       || (!pool_param.has_stride_h() && !pool_param.has_stride_w()))
       << "Stride is stride OR stride_h and stride_w are required.";
+  CHECK((!pool_param.has_kstride() && pool_param.has_kstride_h()
+      && pool_param.has_kstride_w())
+      || (!pool_param.has_kstride_h() && !pool_param.has_kstride_w()))
+      << "KStride is kstride OR kstride_h and kstride_w are required.";
+
   global_pooling_ = pool_param.global_pooling();
   if (global_pooling_) {
     kernel_h_ = bottom[0]->height();
@@ -72,15 +77,28 @@ void PoolingLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,
         == PoolingParameter_PoolMethod_AVE
         || this->layer_param_.pooling_param().pool()
         == PoolingParameter_PoolMethod_MAX)
-        << "Padding implemented only for average and max pooling.";
+      << "Padding implemented only for average and max pooling.";
     CHECK_LT(pad_h_, kernel_h_);
     CHECK_LT(pad_w_, kernel_w_);
   }
+  if (!pool_param.has_kstride_h()) {
+    kstride_h_ = kstride_w_ = pool_param.kstride();
+  } else {
+    kstride_h_ = pool_param.kstride_h();
+    kstride_w_ = pool_param.kstride_w();
+  }
+  if (!(kstride_h_ == 1) || !(kstride_w_ == 1)) {
+    CHECK_EQ(stride_h_, 1) << "Currently, when using kstride, the stride parameter should be fixed to 1.";
+    CHECK_EQ(stride_w_, 1) << "Currently, when using kstride, the stride parameter should be fixed to 1.";
+    CHECK(pool_param.pool() == PoolingParameter_PoolMethod_MAX) << "Only max pooling is impelmented for FCN.";
+  }
+  ext_kernel_h_ = (kernel_h_ - 1) * kstride_h_ + 1;
+  ext_kernel_w_ = (kernel_w_ - 1) * kstride_w_ + 1;
 }
 
 template <typename Dtype>
 void PoolingLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
-      const vector<Blob<Dtype>*>& top) {
+    const vector<Blob<Dtype>*>& top) {
   channels_ = bottom[0]->channels();
   height_ = bottom[0]->height();
   width_ = bottom[0]->width();
@@ -89,9 +107,9 @@ void PoolingLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
     kernel_w_ = bottom[0]->width();
   }
   pooled_height_ = static_cast<int>(ceil(static_cast<float>(
-      height_ + 2 * pad_h_ - kernel_h_) / stride_h_)) + 1;
+	  height_ + 2 * pad_h_ - ext_kernel_h_) / stride_h_)) + 1;
   pooled_width_ = static_cast<int>(ceil(static_cast<float>(
-      width_ + 2 * pad_w_ - kernel_w_) / stride_w_)) + 1;
+	  width_ + 2 * pad_w_ - ext_kernel_w_) / stride_w_)) + 1;
   if (pad_h_ || pad_w_) {
     // If we have padding, ensure that the last pooling starts strictly
     // inside the image (instead of at the padding); otherwise clip the last.
@@ -113,13 +131,13 @@ void PoolingLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
   if (this->layer_param_.pooling_param().pool() ==
       PoolingParameter_PoolMethod_MAX && top.size() == 1) {
     max_idx_.Reshape(bottom[0]->num(), channels_, pooled_height_,
-        pooled_width_);
+	pooled_width_);
   }
   // If stochastic pooling, we will initialize the random index part.
   if (this->layer_param_.pooling_param().pool() ==
       PoolingParameter_PoolMethod_STOCHASTIC) {
     rand_idx_.Reshape(bottom[0]->num(), channels_, pooled_height_,
-      pooled_width_);
+	pooled_width_);
   }
 }
 
@@ -127,7 +145,8 @@ void PoolingLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
 // case?
 template <typename Dtype>
 void PoolingLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
-      const vector<Blob<Dtype>*>& top) {
+    const vector<Blob<Dtype>*>& top) {
+  CHECK((kstride_h_ == 1) && (kstride_w_ == 1)) << "Forward_cpu is not implemented for FCN pooling.";
   const Dtype* bottom_data = bottom[0]->cpu_data();
   Dtype* top_data = top[0]->mutable_cpu_data();
   const int top_count = top[0]->count();
@@ -138,98 +157,99 @@ void PoolingLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
   // Different pooling methods. We explicitly do the switch outside the for
   // loop to save time, although this results in more code.
   switch (this->layer_param_.pooling_param().pool()) {
-  case PoolingParameter_PoolMethod_MAX:
-    // Initialize
-    if (use_top_mask) {
-      top_mask = top[1]->mutable_cpu_data();
-      caffe_set(top_count, Dtype(-1), top_mask);
-    } else {
-      mask = max_idx_.mutable_cpu_data();
-      caffe_set(top_count, -1, mask);
-    }
-    caffe_set(top_count, Dtype(-FLT_MAX), top_data);
-    // The main loop
-    for (int n = 0; n < bottom[0]->num(); ++n) {
-      for (int c = 0; c < channels_; ++c) {
-        for (int ph = 0; ph < pooled_height_; ++ph) {
-          for (int pw = 0; pw < pooled_width_; ++pw) {
-            int hstart = ph * stride_h_ - pad_h_;
-            int wstart = pw * stride_w_ - pad_w_;
-            int hend = min(hstart + kernel_h_, height_);
-            int wend = min(wstart + kernel_w_, width_);
-            hstart = max(hstart, 0);
-            wstart = max(wstart, 0);
-            const int pool_index = ph * pooled_width_ + pw;
-            for (int h = hstart; h < hend; ++h) {
-              for (int w = wstart; w < wend; ++w) {
-                const int index = h * width_ + w;
-                if (bottom_data[index] > top_data[pool_index]) {
-                  top_data[pool_index] = bottom_data[index];
-                  if (use_top_mask) {
-                    top_mask[pool_index] = static_cast<Dtype>(index);
-                  } else {
-                    mask[pool_index] = index;
-                  }
-                }
-              }
-            }
-          }
-        }
-        // compute offset
-        bottom_data += bottom[0]->offset(0, 1);
-        top_data += top[0]->offset(0, 1);
-        if (use_top_mask) {
-          top_mask += top[0]->offset(0, 1);
-        } else {
-          mask += top[0]->offset(0, 1);
-        }
+    case PoolingParameter_PoolMethod_MAX:
+      // Initialize
+      if (use_top_mask) {
+	top_mask = top[1]->mutable_cpu_data();
+	caffe_set(top_count, Dtype(-1), top_mask);
+      } else {
+	mask = max_idx_.mutable_cpu_data();
+	caffe_set(top_count, -1, mask);
       }
-    }
-    break;
-  case PoolingParameter_PoolMethod_AVE:
-    for (int i = 0; i < top_count; ++i) {
-      top_data[i] = 0;
-    }
-    // The main loop
-    for (int n = 0; n < bottom[0]->num(); ++n) {
-      for (int c = 0; c < channels_; ++c) {
-        for (int ph = 0; ph < pooled_height_; ++ph) {
-          for (int pw = 0; pw < pooled_width_; ++pw) {
-            int hstart = ph * stride_h_ - pad_h_;
-            int wstart = pw * stride_w_ - pad_w_;
-            int hend = min(hstart + kernel_h_, height_ + pad_h_);
-            int wend = min(wstart + kernel_w_, width_ + pad_w_);
-            int pool_size = (hend - hstart) * (wend - wstart);
-            hstart = max(hstart, 0);
-            wstart = max(wstart, 0);
-            hend = min(hend, height_);
-            wend = min(wend, width_);
-            for (int h = hstart; h < hend; ++h) {
-              for (int w = wstart; w < wend; ++w) {
-                top_data[ph * pooled_width_ + pw] +=
-                    bottom_data[h * width_ + w];
-              }
-            }
-            top_data[ph * pooled_width_ + pw] /= pool_size;
-          }
-        }
-        // compute offset
-        bottom_data += bottom[0]->offset(0, 1);
-        top_data += top[0]->offset(0, 1);
+      caffe_set(top_count, Dtype(-FLT_MAX), top_data);
+      // The main loop
+      for (int n = 0; n < bottom[0]->num(); ++n) {
+	for (int c = 0; c < channels_; ++c) {
+	  for (int ph = 0; ph < pooled_height_; ++ph) {
+	    for (int pw = 0; pw < pooled_width_; ++pw) {
+	      int hstart = ph * stride_h_ - pad_h_;
+	      int wstart = pw * stride_w_ - pad_w_;
+	      int hend = min(hstart + kernel_h_, height_);
+	      int wend = min(wstart + kernel_w_, width_);
+	      hstart = max(hstart, 0);
+	      wstart = max(wstart, 0);
+	      const int pool_index = ph * pooled_width_ + pw;
+	      for (int h = hstart; h < hend; ++h) {
+		for (int w = wstart; w < wend; ++w) {
+		  const int index = h * width_ + w;
+		  if (bottom_data[index] > top_data[pool_index]) {
+		    top_data[pool_index] = bottom_data[index];
+		    if (use_top_mask) {
+		      top_mask[pool_index] = static_cast<Dtype>(index);
+		    } else {
+		      mask[pool_index] = index;
+		    }
+		  }
+		}
+	      }
+	    }
+	  }
+	  // compute offset
+	  bottom_data += bottom[0]->offset(0, 1);
+	  top_data += top[0]->offset(0, 1);
+	  if (use_top_mask) {
+	    top_mask += top[0]->offset(0, 1);
+	  } else {
+	    mask += top[0]->offset(0, 1);
+	  }
+	}
       }
-    }
-    break;
-  case PoolingParameter_PoolMethod_STOCHASTIC:
-    NOT_IMPLEMENTED;
-    break;
-  default:
-    LOG(FATAL) << "Unknown pooling method.";
+      break;
+    case PoolingParameter_PoolMethod_AVE:
+      for (int i = 0; i < top_count; ++i) {
+	top_data[i] = 0;
+      }
+      // The main loop
+      for (int n = 0; n < bottom[0]->num(); ++n) {
+	for (int c = 0; c < channels_; ++c) {
+	  for (int ph = 0; ph < pooled_height_; ++ph) {
+	    for (int pw = 0; pw < pooled_width_; ++pw) {
+	      int hstart = ph * stride_h_ - pad_h_;
+	      int wstart = pw * stride_w_ - pad_w_;
+	      int hend = min(hstart + kernel_h_, height_ + pad_h_);
+	      int wend = min(wstart + kernel_w_, width_ + pad_w_);
+	      int pool_size = (hend - hstart) * (wend - wstart);
+	      hstart = max(hstart, 0);
+	      wstart = max(wstart, 0);
+	      hend = min(hend, height_);
+	      wend = min(wend, width_);
+	      for (int h = hstart; h < hend; ++h) {
+		for (int w = wstart; w < wend; ++w) {
+		  top_data[ph * pooled_width_ + pw] +=
+		    bottom_data[h * width_ + w];
+		}
+	      }
+	      top_data[ph * pooled_width_ + pw] /= pool_size;
+	    }
+	  }
+	  // compute offset
+	  bottom_data += bottom[0]->offset(0, 1);
+	  top_data += top[0]->offset(0, 1);
+	}
+      }
+      break;
+    case PoolingParameter_PoolMethod_STOCHASTIC:
+      NOT_IMPLEMENTED;
+      break;
+    default:
+      LOG(FATAL) << "Unknown pooling method.";
   }
 }
 
 template <typename Dtype>
 void PoolingLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
-      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {
+    const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {
+  CHECK((kstride_h_ == 1) && (kstride_w_ == 1)) << "Backward_cpu is not implemented for FCN pooling.";
   if (!propagate_down[0]) {
     return;
   }
@@ -243,39 +263,39 @@ void PoolingLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
   const int* mask = NULL;  // suppress warnings about uninitialized variables
   const Dtype* top_mask = NULL;
   switch (this->layer_param_.pooling_param().pool()) {
-  case PoolingParameter_PoolMethod_MAX:
-    // The main loop
-    if (use_top_mask) {
-      top_mask = top[1]->cpu_data();
-    } else {
-      mask = max_idx_.cpu_data();
-    }
-    for (int n = 0; n < top[0]->num(); ++n) {
-      for (int c = 0; c < channels_; ++c) {
-        for (int ph = 0; ph < pooled_height_; ++ph) {
-          for (int pw = 0; pw < pooled_width_; ++pw) {
-            const int index = ph * pooled_width_ + pw;
-            const int bottom_index =
-                use_top_mask ? top_mask[index] : mask[index];
-            bottom_diff[bottom_index] += top_diff[index];
-          }
-        }
-        bottom_diff += bottom[0]->offset(0, 1);
-        top_diff += top[0]->offset(0, 1);
-        if (use_top_mask) {
-          top_mask += top[0]->offset(0, 1);
-        } else {
-          mask += top[0]->offset(0, 1);
-        }
+    case PoolingParameter_PoolMethod_MAX:
+      // The main loop
+      if (use_top_mask) {
+	top_mask = top[1]->cpu_data();
+      } else {
+	mask = max_idx_.cpu_data();
       }
-    }
-    break;
-  case PoolingParameter_PoolMethod_AVE:
-    // The main loop
-    for (int n = 0; n < top[0]->num(); ++n) {
-      for (int c = 0; c < channels_; ++c) {
-        for (int ph = 0; ph < pooled_height_; ++ph) {
-          for (int pw = 0; pw < pooled_width_; ++pw) {
+      for (int n = 0; n < top[0]->num(); ++n) {
+	for (int c = 0; c < channels_; ++c) {
+	  for (int ph = 0; ph < pooled_height_; ++ph) {
+	    for (int pw = 0; pw < pooled_width_; ++pw) {
+	      const int index = ph * pooled_width_ + pw;
+	      const int bottom_index =
+		use_top_mask ? top_mask[index] : mask[index];
+	      bottom_diff[bottom_index] += top_diff[index];
+	    }
+	  }
+	  bottom_diff += bottom[0]->offset(0, 1);
+	  top_diff += top[0]->offset(0, 1);
+	  if (use_top_mask) {
+	    top_mask += top[0]->offset(0, 1);
+	  } else {
+	    mask += top[0]->offset(0, 1);
+	  }
+	}
+      }
+      break;
+    case PoolingParameter_PoolMethod_AVE:
+      // The main loop
+      for (int n = 0; n < top[0]->num(); ++n) {
+	for (int c = 0; c < channels_; ++c) {
+	  for (int ph = 0; ph < pooled_height_; ++ph) {
+	    for (int pw = 0; pw < pooled_width_; ++pw) {
             int hstart = ph * stride_h_ - pad_h_;
             int wstart = pw * stride_w_ - pad_w_;
             int hend = min(hstart + kernel_h_, height_ + pad_h_);
